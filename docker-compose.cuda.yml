version: "3.8"

services:
  whisperx-api-cuda:
    image: ghcr.io/xudongcc/whisperx-api:cuda
    # 如果需要本地构建，取消注释以下行：
    # build:
    #   context: .
    #   dockerfile: Dockerfile.cuda
    #   target: production
    container_name: whisperx-api-cuda
    ports:
      - "8000:8000"
    environment:
      # 服务器配置
      HOST: 0.0.0.0
      PORT: 8000

      # HuggingFace Token (用于说话人识别)
      HF_TOKEN: ${HF_TOKEN:-}

      # 文件配置
      MAX_FILE_SIZE: ${MAX_FILE_SIZE:-100}

      # 日志配置
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      DEBUG: ${DEBUG:-false}

      # NVIDIA 相关环境变量
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility

    volumes:
      # 持久化模型缓存
      - whisperx_models_cuda:/home/whisperx/.cache

      # 挂载日志目录
      - ./logs:/app/logs

      # 挂载临时文件目录
      - ./temp:/app/temp

    restart: unless-stopped

    # CUDA 运行时配置
    runtime: nvidia

    # 健康检查
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s # CUDA 启动可能需要更长时间

    # 资源限制和 CUDA 配置
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 8G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  whisperx_models_cuda:
    driver: local
